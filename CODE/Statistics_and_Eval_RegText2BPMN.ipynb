{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "kVZtQcVbceh8",
        "p9fyaKN6cjvf",
        "_brvNe79xshP",
        "18_APdQKeFpk",
        "hOLS3a-heKUj",
        "6b1dtAan-xOH"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# I. Input Statistics"
      ],
      "metadata": {
        "id": "rkPYAzVOpzte"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text statistics"
      ],
      "metadata": {
        "id": "kVZtQcVbceh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 nltk\n"
      ],
      "metadata": {
        "id": "3sFNKiLRgxw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "nBgQmKP-heUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ScytcppgpPP"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "1. Member States shall require that verification of the identity of the customer and the beneficial owner take place\n",
        "before the establishment of a business relationship or the carrying out of the transaction.\n",
        "2. By way of derogation from paragraph 1, Member States may allow verification of the identity of the customer and the beneficial owner to be completed during the establishment of a business relationship if necessary so as not to interrupt the normal conduct of business and where there is little risk of money laundering or terrorist financing. In such situations, those procedures shall be completed as soon as practicable after initial contact.\n",
        "3. By way of derogation from paragraph 1, Member States may allow the opening of an account with a credit\n",
        "institution or financial institution, including accounts that permit transactions in transferable securities, provided that there are adequate safeguards in place to ensure that transactions are not carried out by the customer or on its behalf until full compliance with the customer due diligence requirements laid down in points (a) and (b) of the first subparagraph of Article 13(1) is obtained.\n",
        "4. Member States shall require that, where an obliged entity is unable to comply with the customer due diligence\n",
        "requirements laid down in point (a), (b) or (c) of the first subparagraph of Article 13(1), it shall not carry out a\n",
        "transaction through a bank account, establish a business relationship or carry out the transaction, and shall terminate the business relationship and consider making a suspicious transaction report to the FIU in relation to the customer in accordance with Article 33. Member States shall not apply the first subparagraph to notaries, other independent legal professionals, auditors, external accountants and tax advisors only to the strict extent that those persons ascertain the legal position of their client, or perform the task of defending or representing that client in, or concerning, judicial proceedings, including providing advice on instituting or avoiding such proceedings.\n",
        "5. Member States shall require that obliged entities apply the customer due diligence measures not only to all new\n",
        "customers but also at appropriate times to existing customers on a risk-sensitive basis, including at times when the\n",
        "relevant circumstances of a customer change.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Count the total number of tokens\n",
        "total_tokens = len(tokens)\n",
        "\n",
        "# Split the text into sentences and count tokens for each sentence\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "sentence_token_counts = [len(nltk.word_tokenize(sentence)) for sentence in sentences]\n",
        "\n",
        "# Calculate average sentence length (in tokens)\n",
        "average_sentence_length_tokens = total_tokens / len(sentences)\n",
        "\n",
        "print(f\"Total tokens: {total_tokens}\")\n",
        "print(f\"Total sentences: {len(sentences)}\")\n",
        "print(f\"Average sentence length (tokens): {average_sentence_length_tokens}\")\n",
        "print(f\"Sentence token counts: {sentence_token_counts}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import PyPDF2\n",
        "\n",
        "# Function to extract text from a PDF file\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, \"rb\") as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Path to your PDF file\n",
        "pdf_path = \"2_blood_donor_selection.pdf\"  # Replace with your PDF file path\n",
        "\n",
        "# Extract text from the PDF\n",
        "text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Count the total number of tokens\n",
        "total_tokens = len(tokens)\n",
        "\n",
        "# Split the text into sentences and count tokens for each sentence\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "sentence_token_counts = [len(nltk.word_tokenize(sentence)) for sentence in sentences]\n",
        "\n",
        "# Calculate average sentence length (in tokens)\n",
        "average_sentence_length_tokens = total_tokens / len(sentences)\n",
        "\n",
        "# Output the results\n",
        "print(f\"Total tokens: {total_tokens}\")\n",
        "print(f\"Total sentences: {len(sentences)}\")\n",
        "print(f\"Average sentence length (tokens): {average_sentence_length_tokens}\")\n",
        "print(f\"Sentence token counts: {sentence_token_counts}\")"
      ],
      "metadata": {
        "id": "FY3S5AZPnnQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model statistics"
      ],
      "metadata": {
        "id": "p9fyaKN6cjvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "\n",
        "def analyze_bpmn(file_path):\n",
        "    \"\"\"\n",
        "    Parses a .bpmn file to count nodes (activities and events),\n",
        "    gateways, edges (sequence flows), and roles (lanes).\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The full path to the .bpmn file.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the counts of each element type.\n",
        "              Returns None if the file is not found or is not a valid XML.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"Error: File not found at '{file_path}'\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        tree = ET.parse(file_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        namespace = {'bpmn': root.tag.split('}')[0][1:]}\n",
        "\n",
        "        activity_types = [\n",
        "            'task', 'sendTask', 'receiveTask', 'userTask', 'manualTask',\n",
        "            'businessRuleTask', 'serviceTask', 'scriptTask', 'callActivity', 'subProcess'\n",
        "        ]\n",
        "        event_types = [\n",
        "            'startEvent', 'endEvent', 'intermediateThrowEvent',\n",
        "            'intermediateCatchEvent', 'boundaryEvent'\n",
        "        ]\n",
        "\n",
        "        gateway_types = [\n",
        "            'exclusiveGateway', 'parallelGateway', 'inclusiveGateway',\n",
        "            'eventBasedGateway', 'complexGateway'\n",
        "        ]\n",
        "\n",
        "        edge_types = ['sequenceFlow']\n",
        "\n",
        "        role_types = ['pool','lane']\n",
        "\n",
        "        total_activities = sum(len(root.findall(f\".//bpmn:{t}\", namespace)) for t in activity_types)\n",
        "\n",
        "        total_events = sum(len(root.findall(f\".//bpmn:{t}\", namespace)) for t in event_types)\n",
        "\n",
        "        total_nodes = total_activities + total_events\n",
        "\n",
        "        total_gateways = sum(len(root.findall(f\".//bpmn:{t}\", namespace)) for t in gateway_types)\n",
        "\n",
        "        total_edges = sum(len(root.findall(f\".//bpmn:{t}\", namespace)) for t in edge_types)\n",
        "\n",
        "        total_roles = sum(len(root.findall(f\".//bpmn:{t}\", namespace)) for t in role_types)\n",
        "\n",
        "        if total_roles == 0:\n",
        "            total_roles = len(root.findall('.//bpmn:participant', namespace))\n",
        "\n",
        "        counts = {\n",
        "            \"Total Nodes (Activities + Events)\": total_nodes,\n",
        "            \"Total Gateways\": total_gateways,\n",
        "            \"Total Edges (Sequence Flows)\": total_edges,\n",
        "            \"Total Roles (Lanes/Participants)\": total_roles\n",
        "        }\n",
        "\n",
        "        return counts\n",
        "\n",
        "    except ET.ParseError:\n",
        "        print(f\"Error: Could not parse '{file_path}'. Make sure it is a valid BPMN/XML file.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "\n",
        "    file_path = \"/content/finance_customer_due_diligence.bpmn\"\n",
        "    analysis_results = analyze_bpmn(file_path)\n",
        "\n",
        "    if analysis_results:\n",
        "        print(\"\\n--- BPMN Analysis Results ---\")\n",
        "        for key, value in analysis_results.items():\n",
        "            print(f\"{key}: {value}\")\n",
        "        print(\"---------------------------\\n\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "pBrj_ElWooFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II. Semantic EVAL\n"
      ],
      "metadata": {
        "id": "xOXEOfKiprpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Completeness"
      ],
      "metadata": {
        "id": "_brvNe79xshP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers torch pandas openpyxl"
      ],
      "metadata": {
        "id": "qnhOsQFYKAVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step one check for completness\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "NAMESPACES = {'bpmn': 'http://www.omg.org/spec/BPMN/20100524/MODEL'}\n",
        "\n",
        "\n",
        "def get_elements_by_tag(root, tag_name):\n",
        "    \"\"\"\n",
        "    Finds all elements with a specific tag name in the BPMN XML tree.\n",
        "\n",
        "    Args:\n",
        "        root (ET.Element): The root of the XML tree.\n",
        "        tag_name (str): The BPMN tag to search for (e.g., 'task', 'userTask').\n",
        "\n",
        "    Returns:\n",
        "        list: A list of found XML elements.\n",
        "    \"\"\"\n",
        "    return root.findall(f'.//bpmn:{tag_name}', NAMESPACES)\n",
        "\n",
        "def get_element_names(elements):\n",
        "    \"\"\"\n",
        "    Extracts the 'name' attribute from a list of BPMN elements.\n",
        "    Filters out elements without a name.\n",
        "\n",
        "    Args:\n",
        "        elements (list): A list of XML elements.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of names (strings).\n",
        "    \"\"\"\n",
        "    names = [elem.get('name') for elem in elements]\n",
        "    return [name for name in names if name]\n",
        "\n",
        "def get_actors(root):\n",
        "    \"\"\"\n",
        "    Extracts actors from the BPMN model. Actors are represented as\n",
        "    Participants (which correspond to Pools) and Lanes.\n",
        "\n",
        "    Args:\n",
        "        root (ET.Element): The root of the XML tree.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of actor names.\n",
        "    \"\"\"\n",
        "    participants = get_elements_by_tag(root, 'participant')\n",
        "    lanes = get_elements_by_tag(root, 'lane')\n",
        "    return get_element_names(participants + lanes)\n",
        "\n",
        "def get_activities(root):\n",
        "    \"\"\"\n",
        "    Extracts activities from the BPMN model.\n",
        "    This includes tasks, user tasks, service tasks, etc.\n",
        "\n",
        "    Args:\n",
        "        root (ET.Element): The root of the XML tree.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of activity names.\n",
        "    \"\"\"\n",
        "    activity_tags = ['task', 'userTask', 'serviceTask', 'sendTask', 'receiveTask',\n",
        "                     'manualTask', 'businessRuleTask', 'scriptTask', 'callActivity', 'subProcess']\n",
        "    activities = []\n",
        "    for tag in activity_tags:\n",
        "        activities.extend(get_elements_by_tag(root, tag))\n",
        "    return get_element_names(activities)\n",
        "\n",
        "def get_events(root):\n",
        "    \"\"\"\n",
        "    Extracts events from the BPMN model (start, end, intermediate).\n",
        "\n",
        "    Args:\n",
        "        root (ET.Element): The root of the XML tree.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of event names.\n",
        "    \"\"\"\n",
        "    event_tags = ['startEvent', 'endEvent', 'intermediateThrowEvent', 'intermediateCatchEvent', 'boundaryEvent']\n",
        "    events = []\n",
        "    for tag in event_tags:\n",
        "        events.extend(get_elements_by_tag(root, tag))\n",
        "    return get_element_names(events)\n",
        "\n",
        "def get_gateways_count(root):\n",
        "    \"\"\"\n",
        "    Counts the number of AND (parallel) and XOR (exclusive) gateways.\n",
        "    These are often not named, so we count them instead of matching names.\n",
        "\n",
        "    Args:\n",
        "        root (ET.Element): The root of the XML tree.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with counts for 'AND' and 'XOR' gateways.\n",
        "    \"\"\"\n",
        "    and_gateways = get_elements_by_tag(root, 'parallelGateway')\n",
        "    xor_gateways = get_elements_by_tag(root, 'exclusiveGateway')\n",
        "    return {'AND': len(and_gateways), 'XOR': len(xor_gateways)}\n",
        "\n",
        "def get_data_objects(root):\n",
        "    \"\"\"\n",
        "    Extracts data objects from the BPMN model.\n",
        "\n",
        "    Args:\n",
        "        root (ET.Element): The root of the XML tree.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of data object names.\n",
        "    \"\"\"\n",
        "    data_objects = get_elements_by_tag(root, 'dataObjectReference')\n",
        "    return get_element_names(data_objects)\n",
        "\n",
        "def get_conditions(root):\n",
        "    \"\"\"\n",
        "    Extracts conditions, defined as the names/labels of the\n",
        "    XOR (exclusive) gateways that represent a decision split.\n",
        "\n",
        "    Args:\n",
        "        root (ET.Element): The root of the XML tree.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of condition texts (gateway labels).\n",
        "    \"\"\"\n",
        "    conditions = []\n",
        "    xor_gateways = root.findall('.//bpmn:exclusiveGateway', NAMESPACES)\n",
        "\n",
        "    for gateway in xor_gateways:\n",
        "        gateway_id = gateway.get('id')\n",
        "        outgoing_flows = [\n",
        "            flow for flow in root.findall('.//bpmn:sequenceFlow', NAMESPACES)\n",
        "            if flow.get('sourceRef') == gateway_id\n",
        "        ]\n",
        "\n",
        "        if len(outgoing_flows) > 1:\n",
        "            condition_text = gateway.get('name')\n",
        "            if condition_text:\n",
        "                conditions.append(condition_text)\n",
        "\n",
        "    return conditions\n",
        "\n",
        "\n",
        "def calculate_recall_for_semantic_elements(gold_standard_names, generated_names, similarity_threshold, model):\n",
        "    \"\"\"\n",
        "    Calculates recall for named elements based on semantic similarity and returns detailed match lists.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing (recall, matches_count, total_gold, matches_list, unmatched_gold, unmatched_generated).\n",
        "    \"\"\"\n",
        "    if not gold_standard_names:\n",
        "        return 1.0, 0, 0, [], [], generated_names\n",
        "    if not generated_names:\n",
        "        return 0.0, 0, len(gold_standard_names), [], gold_standard_names, []\n",
        "\n",
        "    gold_embeddings = model.encode(gold_standard_names, convert_to_tensor=True)\n",
        "    generated_embeddings = model.encode(generated_names, convert_to_tensor=True)\n",
        "\n",
        "    cosine_scores = util.pytorch_cos_sim(gold_embeddings, generated_embeddings)\n",
        "\n",
        "    matches_list = []\n",
        "    unmatched_gold_indices = set(range(len(gold_standard_names)))\n",
        "    unmatched_generated_indices = set(range(len(generated_names)))\n",
        "\n",
        "    matched_generated_indices = set()\n",
        "\n",
        "    for gs_idx in range(len(gold_standard_names)):\n",
        "        best_score = -1\n",
        "        best_gen_idx = -1\n",
        "        for gen_idx in range(len(generated_names)):\n",
        "            if gen_idx in matched_generated_indices:\n",
        "                continue\n",
        "\n",
        "            score = cosine_scores[gs_idx][gen_idx]\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_gen_idx = gen_idx\n",
        "\n",
        "        if best_score >= similarity_threshold:\n",
        "            matches_list.append(\n",
        "                (gold_standard_names[gs_idx], generated_names[best_gen_idx], best_score.item())\n",
        "            )\n",
        "            if best_gen_idx != -1:\n",
        "                matched_generated_indices.add(best_gen_idx)\n",
        "\n",
        "            if gs_idx in unmatched_gold_indices:\n",
        "                unmatched_gold_indices.remove(gs_idx)\n",
        "            if best_gen_idx in unmatched_generated_indices:\n",
        "                unmatched_generated_indices.remove(best_gen_idx)\n",
        "\n",
        "    matches_count = len(matches_list)\n",
        "    total_gold = len(gold_standard_names)\n",
        "    recall = matches_count / total_gold if total_gold > 0 else 1.0\n",
        "\n",
        "    unmatched_gold = [gold_standard_names[i] for i in unmatched_gold_indices]\n",
        "    unmatched_generated = [generated_names[i] for i in unmatched_generated_indices]\n",
        "\n",
        "    return recall, matches_count, total_gold, matches_list, unmatched_gold, unmatched_generated\n",
        "\n",
        "\n",
        "def calculate_recall_for_counts(gold_standard_count, generated_count):\n",
        "    \"\"\"\n",
        "    Calculates recall for elements that are counted (like gateways).\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing (recall, matches, total_count).\n",
        "    \"\"\"\n",
        "    if gold_standard_count == 0:\n",
        "        return 1.0, 0, 0\n",
        "\n",
        "    matches = min(gold_standard_count, generated_count)\n",
        "    recall = matches / gold_standard_count\n",
        "    return recall, matches, gold_standard_count\n",
        "\n",
        "def format_matches_for_excel(matches_list, unmatched_gold, unmatched_gen):\n",
        "    \"\"\"Formats the detailed match lists into a single string for an Excel cell.\"\"\"\n",
        "    report_parts = []\n",
        "    if matches_list:\n",
        "        report_parts.append(\"MATCHED:\")\n",
        "        for gs, gen, score in sorted(matches_list, key=lambda x: x[2], reverse=True):\n",
        "            report_parts.append(f\"  - G: '{gs}' <-> M: '{gen}' ({score:.2f})\")\n",
        "    if unmatched_gold:\n",
        "        report_parts.append(\"\\nMISSING FROM MODEL:\")\n",
        "        for name in sorted(unmatched_gold):\n",
        "            report_parts.append(f\"  - {name}\")\n",
        "    if unmatched_gen:\n",
        "        report_parts.append(\"\\nEXTRA IN MODEL:\")\n",
        "        for name in sorted(unmatched_gen):\n",
        "            report_parts.append(f\"  - {name}\")\n",
        "    return \"\\n\".join(report_parts)\n",
        "\n",
        "\n",
        "def run_completeness_check(gold_standard_file, generated_file, thresholds, model):\n",
        "    \"\"\"\n",
        "    Main function to run the completeness check.\n",
        "    Returns a dictionary with all the results for a single comparison.\n",
        "    \"\"\"\n",
        "    results = {\n",
        "        'Gold Standard': os.path.basename(gold_standard_file),\n",
        "        'Generated Model': os.path.basename(generated_file),\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        gold_tree = ET.parse(gold_standard_file)\n",
        "        gold_root = gold_tree.getroot()\n",
        "    except (FileNotFoundError, ET.ParseError) as e:\n",
        "        print(f\"Error reading gold standard file '{gold_standard_file}': {e}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        generated_tree = ET.parse(generated_file)\n",
        "        generated_root = generated_tree.getroot()\n",
        "    except (FileNotFoundError, ET.ParseError) as e:\n",
        "        print(f\"Error reading generated file '{generated_file}': {e}\")\n",
        "        return None\n",
        "\n",
        "    all_elements = {\n",
        "        'Actors': (get_actors(gold_root), get_actors(generated_root)),\n",
        "        'Activities': (get_activities(gold_root), get_activities(generated_root)),\n",
        "        'Events': (get_events(gold_root), get_events(generated_root)),\n",
        "        'Data Objects': (get_data_objects(gold_root), get_data_objects(generated_root)),\n",
        "        'Conditions': (get_conditions(gold_root), get_conditions(generated_root)),\n",
        "    }\n",
        "\n",
        "    for category, (gold_list, gen_list) in all_elements.items():\n",
        "        thresh_key = category.lower().replace(' ', '_')\n",
        "        thresh = thresholds.get(thresh_key, 0.75)\n",
        "        recall, matches, total, matches_list, unmatched_gold, unmatched_gen = \\\n",
        "            calculate_recall_for_semantic_elements(gold_list, gen_list, thresh, model)\n",
        "\n",
        "        results[f'{category} Recall'] = recall\n",
        "        results[f'{category} Absolute'] = f\"{matches}/{total}\"\n",
        "        results[f'{category} Details'] = format_matches_for_excel(matches_list, unmatched_gold, unmatched_gen)\n",
        "\n",
        "    gold_gateways = get_gateways_count(gold_root)\n",
        "    gen_gateways = get_gateways_count(generated_root)\n",
        "\n",
        "    recall_and, matches_and, total_and = calculate_recall_for_counts(gold_gateways['AND'], gen_gateways['AND'])\n",
        "    results['AND Gateways Recall'] = recall_and\n",
        "    results['AND Gateways Absolute'] = f\"{matches_and}/{total_and}\"\n",
        "    results['AND Gateways Details'] = f\"Gold: {gold_gateways['AND']}, Generated: {gen_gateways['AND']}\"\n",
        "\n",
        "    recall_xor, matches_xor, total_xor = calculate_recall_for_counts(gold_gateways['XOR'], gen_gateways['XOR'])\n",
        "    results['XOR Gateways Recall'] = recall_xor\n",
        "    results['XOR Gateways Absolute'] = f\"{matches_xor}/{total_xor}\"\n",
        "    results['XOR Gateways Details'] = f\"Gold: {gold_gateways['XOR']}, Generated: {gen_gateways['XOR']}\"\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # file pairs to compare: (gold_standard_file, generated_file)\n",
        "    comparison_files = [\n",
        "        (\"/content/smart_meter_refactored.bpmn\", \"/content/smart_meter_01.bpmn\"),\n",
        "        (\"/content/smart_meter_refactored.bpmn\", \"/content/smart_meter_02.bpmn\"),\n",
        "        (\"/content/smart_meter_refactored.bpmn\", \"/content/smart_meter_03.bpmn\"),\n",
        "        (\"/content/gdpr_refactored.bpmn\", \"/content/gdpr_01.bpmn\"),\n",
        "        (\"/content/gdpr_refactored.bpmn\", \"/content/gdpr_02.bpmn\"),\n",
        "        (\"/content/gdpr_refactored.bpmn\", \"/content/gdpr_03.bpmn\"),\n",
        "        (\"/content/blood_donor_selection.bpmn\", \"/content/blood_donor_01.bpmn\"),\n",
        "        (\"/content/blood_donor_selection.bpmn\", \"/content/blood_donor_02.bpmn\"),\n",
        "        (\"/content/blood_donor_selection.bpmn\", \"/content/blood_donor_03.bpmn\"),\n",
        "        (\"/content/health_data.bpmn\", \"/content/health_data_01.bpmn\"),\n",
        "        (\"/content/health_data.bpmn\", \"/content/health_data_02.bpmn\"),\n",
        "        (\"/content/health_data.bpmn\", \"/content/health_data_03.bpmn\"),\n",
        "        (\"/content/finance_customer_due_diligence.bpmn\", \"/content/CDD_01.bpmn\"),\n",
        "        (\"/content/finance_customer_due_diligence.bpmn\", \"/content/CDD_02.bpmn\"),\n",
        "        (\"/content/finance_customer_due_diligence.bpmn\", \"/content/CDD_03.bpmn\")\n",
        "    ]\n",
        "\n",
        "    output_excel_file = \"bpmn_completeness_report_CARB.xlsx\"\n",
        "\n",
        "    similarity_thresholds = {\n",
        "        'actors': 0.70,\n",
        "        'activities': 0.50,\n",
        "        'events': 0.55,\n",
        "        'data_objects': 0.60,\n",
        "        'conditions': 0.60\n",
        "    }\n",
        "\n",
        "    print(\"Loading semantic similarity model (this may take a moment)...\")\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "    all_results = []\n",
        "    print(\"\\nStarting batch processing...\")\n",
        "    for gold_file, generated_file in comparison_files:\n",
        "        print(f\"  - Comparing '{os.path.basename(gold_file)}' with '{os.path.basename(generated_file)}'\")\n",
        "        result_data = run_completeness_check(gold_file, generated_file, similarity_thresholds, model)\n",
        "        if result_data:\n",
        "            all_results.append(result_data)\n",
        "\n",
        "    if all_results:\n",
        "        df = pd.DataFrame(all_results)\n",
        "\n",
        "        column_order = [\n",
        "            'Gold Standard', 'Generated Model',\n",
        "            'Actors Recall', 'Actors Absolute',\n",
        "            'Activities Recall', 'Activities Absolute',\n",
        "            'Events Recall', 'Events Absolute',\n",
        "            'Data Objects Recall', 'Data Objects Absolute',\n",
        "            'Conditions Recall', 'Conditions Absolute',\n",
        "            'AND Gateways Recall', 'AND Gateways Absolute',\n",
        "            'XOR Gateways Recall', 'XOR Gateways Absolute',\n",
        "            'Actors Details', 'Activities Details', 'Events Details',\n",
        "            'Data Objects Details', 'Conditions Details', 'AND Gateways Details', 'XOR Gateways Details'\n",
        "        ]\n",
        "        df = df.reindex(columns=column_order)\n",
        "\n",
        "        try:\n",
        "            df.to_excel(output_excel_file, index=False, engine='openpyxl')\n",
        "            print(f\"\\nSuccessfully exported {len(all_results)} results to '{output_excel_file}'\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError exporting to Excel: {e}\")\n",
        "    else:\n",
        "        print(\"\\nNo results to export.\")"
      ],
      "metadata": {
        "id": "304qhorId1mC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Correctness"
      ],
      "metadata": {
        "id": "18_APdQKeFpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers torch pandas openpyxl requests"
      ],
      "metadata": {
        "id": "Uodv9GqHJAmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step two check for correctness\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "\n",
        "NAMESPACES = {'bpmn': 'http://www.omg.org/spec/BPMN/20100524/MODEL'}\n",
        "\n",
        "class BPMNModel:\n",
        "    \"\"\"\n",
        "    Represents a parsed BPMN model with efficient lookups for elements,\n",
        "    lanes, and participants.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_path):\n",
        "        try:\n",
        "            self.root = ET.parse(file_path).getroot()\n",
        "            # Create maps for efficient lookups\n",
        "            self.id_map = {elem.get('id'): elem for elem in self.root.iter() if elem.get('id')}\n",
        "            self.parent_map = {c: p for p in self.root.iter() for c in p}\n",
        "            self.elements_by_name = self._get_all_elements_by_name()\n",
        "            self.element_to_lane_name_map = self._map_elements_to_lanes()\n",
        "            self.process_to_participant_name_map = self._map_processes_to_participants()\n",
        "        except ET.ParseError as e:\n",
        "            raise IOError(f\"Failed to parse XML file: {file_path}. Error: {e}\")\n",
        "\n",
        "    def _get_all_elements_by_name(self):\n",
        "        \"\"\"Creates a dictionary mapping element names to a list of elements.\"\"\"\n",
        "        mapping = defaultdict(list)\n",
        "        for elem in self.id_map.values():\n",
        "            name = elem.get('name')\n",
        "            if name:\n",
        "                mapping[name].append(elem)\n",
        "        return mapping\n",
        "\n",
        "    def _map_elements_to_lanes(self):\n",
        "        \"\"\"Maps flow element IDs to the name of the lane they belong to.\"\"\"\n",
        "        mapping = {}\n",
        "        for lane in self.root.findall('.//bpmn:lane', NAMESPACES):\n",
        "            lane_name = lane.get('name')\n",
        "            if lane_name:\n",
        "                for node_ref in lane.findall('.//bpmn:flowNodeRef', NAMESPACES):\n",
        "                    if node_ref.text:\n",
        "                        mapping[node_ref.text] = lane_name\n",
        "        return mapping\n",
        "\n",
        "    def _map_processes_to_participants(self):\n",
        "        \"\"\"Maps process IDs to the name of the participant (pool) they belong to.\"\"\"\n",
        "        mapping = {}\n",
        "        for participant in self.root.findall('.//bpmn:participant', NAMESPACES):\n",
        "            participant_name = participant.get('name')\n",
        "            process_ref = participant.get('processRef')\n",
        "            if participant_name and process_ref:\n",
        "                mapping[process_ref] = participant_name\n",
        "        return mapping\n",
        "\n",
        "    def get_all_activities(self):\n",
        "        \"\"\"Returns a list of all activity elements in the model.\"\"\"\n",
        "        activity_tags = ['task', 'userTask', 'serviceTask', 'sendTask', 'receiveTask',\n",
        "                         'manualTask', 'businessRuleTask', 'scriptTask', 'callActivity', 'subProcess']\n",
        "        activities = []\n",
        "        for tag in activity_tags:\n",
        "            activities.extend(self.root.findall(f'.//bpmn:{tag}', NAMESPACES))\n",
        "        return activities\n",
        "\n",
        "    def get_activity_actor_pairs(self):\n",
        "        \"\"\"\n",
        "        Extracts all (activity_name, actor_name) pairs from the model.\n",
        "        \"\"\"\n",
        "        pairs = []\n",
        "        activities = self.get_all_activities()\n",
        "        for activity in activities:\n",
        "            activity_name = activity.get('name')\n",
        "            actor_name = self.get_actor_name_for_element(activity)\n",
        "            if activity_name and actor_name:\n",
        "                pairs.append((activity_name, actor_name))\n",
        "        return pairs\n",
        "\n",
        "    def get_all_flow_nodes(self):\n",
        "        \"\"\"Returns a list of all flow node elements (tasks, events, gateways).\"\"\"\n",
        "        flow_node_tags = [\n",
        "            'task', 'userTask', 'serviceTask', 'sendTask', 'receiveTask',\n",
        "            'manualTask', 'businessRuleTask', 'scriptTask', 'callActivity', 'subProcess',\n",
        "            'startEvent', 'endEvent', 'intermediateThrowEvent', 'intermediateCatchEvent',\n",
        "            'boundaryEvent', 'parallelGateway', 'exclusiveGateway', 'inclusiveGateway',\n",
        "            'eventBasedGateway', 'complexGateway'\n",
        "        ]\n",
        "        nodes = []\n",
        "        for tag in flow_node_tags:\n",
        "            nodes.extend(self.root.findall(f'.//bpmn:{tag}', NAMESPACES))\n",
        "        return nodes\n",
        "\n",
        "    def get_control_flow_pairs(self):\n",
        "        \"\"\"\n",
        "        Extracts all control flow pairs (source_name, target_name) from the model.\n",
        "        \"\"\"\n",
        "        pairs = []\n",
        "        flow_nodes = self.get_all_flow_nodes()\n",
        "        for node in flow_nodes:\n",
        "            source_name = node.get('name')\n",
        "            if not source_name:\n",
        "                continue\n",
        "\n",
        "            outgoing_flows = self.root.findall(f\".//bpmn:sequenceFlow[@sourceRef='{node.get('id')}']\", NAMESPACES)\n",
        "            for flow in outgoing_flows:\n",
        "                target_id = flow.get('targetRef')\n",
        "                if target_id and target_id in self.id_map:\n",
        "                    target_elem = self.id_map[target_id]\n",
        "                    target_name = target_elem.get('name')\n",
        "                    if target_name:\n",
        "                        pairs.append((source_name, target_name))\n",
        "        return pairs\n",
        "\n",
        "    def get_decision_splits(self):\n",
        "        \"\"\"\n",
        "        Extracts all decision splits (XOR gateway name and its conditional labels).\n",
        "        This is the new method for the revised decision logic check.\n",
        "        \"\"\"\n",
        "        splits = []\n",
        "        # Find all exclusive gateways that have a name.\n",
        "        gateways = self.root.findall('.//bpmn:exclusiveGateway[@name]', NAMESPACES)\n",
        "        for gw in gateways:\n",
        "            labels = self.get_outgoing_conditional_flow_labels(gw)\n",
        "            # A decision split must have at least two outgoing conditional flows.\n",
        "            if len(labels) > 1:\n",
        "                splits.append({'name': gw.get('name'), 'labels': labels})\n",
        "        return splits\n",
        "\n",
        "    def get_element_by_name(self, name):\n",
        "        \"\"\"Retrieves the first element matching a given name.\"\"\"\n",
        "        return self.elements_by_name.get(name, [None])[0]\n",
        "\n",
        "    def get_actor_name_for_element(self, element):\n",
        "        \"\"\"\n",
        "        Finds the actor (Lane or Participant name) for a given element.\n",
        "        \"\"\"\n",
        "        if element is None:\n",
        "            return None\n",
        "        elem_id = element.get('id')\n",
        "\n",
        "        if elem_id in self.element_to_lane_name_map:\n",
        "            return self.element_to_lane_name_map[elem_id]\n",
        "\n",
        "        current = element\n",
        "        while current is not None:\n",
        "            parent = self.parent_map.get(current)\n",
        "            if parent is not None and parent.tag.endswith('process'):\n",
        "                process_id = parent.get('id')\n",
        "                if process_id in self.process_to_participant_name_map:\n",
        "                    return self.process_to_participant_name_map[process_id]\n",
        "                break\n",
        "            current = parent\n",
        "        return None\n",
        "\n",
        "    def get_outgoing_conditional_flow_labels(self, element):\n",
        "        \"\"\"Gets the names (labels) of outgoing conditional flows from a gateway.\"\"\"\n",
        "        if element is None:\n",
        "            return []\n",
        "        elem_id = element.get('id')\n",
        "        labels = []\n",
        "        flows = self.root.findall(f\".//bpmn:sequenceFlow[@sourceRef='{elem_id}']\", NAMESPACES)\n",
        "        for flow in flows:\n",
        "            if flow.get('name'):\n",
        "                labels.append(flow.get('name'))\n",
        "        return sorted(labels)\n",
        "\n",
        "def get_semantic_matches(list_a, list_b, threshold, model):\n",
        "    \"\"\"Finds the best semantic matches between two lists of strings.\"\"\"\n",
        "    if not list_a or not list_b:\n",
        "        return []\n",
        "\n",
        "    embeddings_a = model.encode(list_a, convert_to_tensor=True)\n",
        "    embeddings_b = model.encode(list_b, convert_to_tensor=True)\n",
        "    cosine_scores = util.pytorch_cos_sim(embeddings_a, embeddings_b)\n",
        "\n",
        "    matches = []\n",
        "    matched_b_indices = set()\n",
        "\n",
        "    for i, item_a in enumerate(list_a):\n",
        "        best_score = -1\n",
        "        best_j = -1\n",
        "        for j, item_b in enumerate(list_b):\n",
        "            if j in matched_b_indices:\n",
        "                continue\n",
        "            score = cosine_scores[i][j]\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_j = j\n",
        "\n",
        "        if best_score >= threshold:\n",
        "            matches.append({'gold': item_a, 'gen': list_b[best_j], 'score': best_score.item()})\n",
        "            if best_j != -1:\n",
        "                matched_b_indices.add(best_j)\n",
        "    return matches\n",
        "\n",
        "def ask_llm(prompt, api_key):\n",
        "    \"\"\"Sends a prompt to the Gemini API and returns the response.\"\"\"\n",
        "    api_url = f\"https://generativelanguage.googleapis.com/v1/models/gemini-2.5-pro:generateContent?key={api_key}\"\n",
        "\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    payload = {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\n",
        "\n",
        "    max_retries = 3\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(api_url, headers=headers, json=payload, timeout=120)\n",
        "\n",
        "            if 500 <= response.status_code < 600:\n",
        "                print(f\"API Request Warning: Received status {response.status_code}. Retrying in {2**attempt}s...\")\n",
        "                time.sleep(2**attempt)\n",
        "                continue\n",
        "\n",
        "            response.raise_for_status()\n",
        "            response_json = response.json()\n",
        "\n",
        "            if 'candidates' in response_json and len(response_json['candidates']) > 0:\n",
        "                content = response_json['candidates'][0].get('content', {})\n",
        "                if 'parts' in content and len(content['parts']) > 0:\n",
        "                    return content['parts'][0].get('text', '')\n",
        "            return \"Error: Could not parse LLM response.\"\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"API Request Error: {e}. Attempt {attempt + 1} of {max_retries}.\")\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2**attempt)\n",
        "            else:\n",
        "                return \"Error: API request failed after multiple retries.\"\n",
        "    return \"Error: API request failed after multiple retries.\"\n",
        "\n",
        "def describe_model_for_llm(model: BPMNModel):\n",
        "    \"\"\"Creates a textual description of a BPMN model for LLM analysis.\"\"\"\n",
        "    description = []\n",
        "    description.append(\"BPMN Model Structure:\")\n",
        "\n",
        "    participants = model.root.findall('.//bpmn:participant', NAMESPACES)\n",
        "    for p in participants:\n",
        "        p_name = p.get('name')\n",
        "        description.append(f\"\\n- Participant (Pool): '{p_name}'\")\n",
        "\n",
        "        process_ref = p.get('processRef')\n",
        "        process = model.root.find(f\".//bpmn:process[@id='{process_ref}']\", NAMESPACES)\n",
        "        if process is not None:\n",
        "            for elem in process:\n",
        "                elem_name = elem.get('name')\n",
        "                elem_tag = elem.tag.split('}')[-1]\n",
        "                if elem_name:\n",
        "                    description.append(f\"  - Contains {elem_tag}: '{elem_name}'\")\n",
        "\n",
        "    description.append(\"\\n- Sequence Flows (Connections):\")\n",
        "    flows = model.root.findall('.//bpmn:sequenceFlow', NAMESPACES)\n",
        "    for flow in flows:\n",
        "        source_id = flow.get('sourceRef')\n",
        "        target_id = flow.get('targetRef')\n",
        "        source_name = model.id_map.get(source_id, ET.Element(\"\")).get('name', 'Unnamed')\n",
        "        target_name = model.id_map.get(target_id, ET.Element(\"\")).get('name', 'Unnamed')\n",
        "        flow_name = flow.get('name')\n",
        "\n",
        "        flow_desc = f\"  - From '{source_name}' to '{target_name}'\"\n",
        "        if flow_name:\n",
        "            flow_desc += f\" (Condition: '{flow_name}')\"\n",
        "        description.append(flow_desc)\n",
        "\n",
        "    return \"\\n\".join(description)\n",
        "\n",
        "def check_actor_assignment(gold_model, gen_model, model, thresholds):\n",
        "    \"\"\"\n",
        "    Checks for semantically similar (activity, actor) pairs.\n",
        "    \"\"\"\n",
        "    gold_pairs = gold_model.get_activity_actor_pairs()\n",
        "    gen_pairs = gen_model.get_activity_actor_pairs()\n",
        "    total_gold_pairs = len(gold_pairs)\n",
        "\n",
        "    if total_gold_pairs == 0:\n",
        "        return \"NA\" if not gen_pairs else 1.0, \"0/0\", \"Not applicable: No (activity, actor) pairs in gold standard.\"\n",
        "\n",
        "    if not gen_pairs:\n",
        "        return 0.0, f\"0/{total_gold_pairs}\", \"No (activity, actor) pairs found in the generated model.\"\n",
        "\n",
        "    all_activity_names = list(set([p[0] for p in gold_pairs] + [p[0] for p in gen_pairs]))\n",
        "    all_actor_names = list(set([p[1] for p in gold_pairs] + [p[1] for p in gen_pairs]))\n",
        "\n",
        "    activity_embeddings = {name: emb for name, emb in zip(all_activity_names, model.encode(all_activity_names))}\n",
        "    actor_embeddings = {name: emb for name, emb in zip(all_actor_names, model.encode(all_actor_names))}\n",
        "\n",
        "    def get_similarity(name1, name2, embedding_dict):\n",
        "        if name1 not in embedding_dict or name2 not in embedding_dict:\n",
        "            return 0.0\n",
        "        return util.pytorch_cos_sim(embedding_dict[name1], embedding_dict[name2]).item()\n",
        "\n",
        "    matched_gen_indices = set()\n",
        "    correctly_assigned = 0\n",
        "    details = []\n",
        "\n",
        "    for gold_activity, gold_actor in gold_pairs:\n",
        "        best_pair_score = -1\n",
        "        best_gen_idx = -1\n",
        "\n",
        "        for gen_idx, (gen_activity, gen_actor) in enumerate(gen_pairs):\n",
        "            if gen_idx in matched_gen_indices:\n",
        "                continue\n",
        "\n",
        "            sim_activity = get_similarity(gold_activity, gen_activity, activity_embeddings)\n",
        "            sim_actor = get_similarity(gold_actor, gen_actor, actor_embeddings)\n",
        "\n",
        "            if sim_activity >= thresholds['activities'] and sim_actor >= thresholds['actors']:\n",
        "                current_pair_score = (sim_activity + sim_actor) / 2.0\n",
        "                if current_pair_score > best_pair_score:\n",
        "                    best_pair_score = current_pair_score\n",
        "                    best_gen_idx = gen_idx\n",
        "\n",
        "        if best_gen_idx != -1:\n",
        "            correctly_assigned += 1\n",
        "            matched_gen_indices.add(best_gen_idx)\n",
        "            matched_gen_pair = gen_pairs[best_gen_idx]\n",
        "            details.append(f\"MATCH: Gold ('{gold_activity}' by '{gold_actor}') with Gen ('{matched_gen_pair[0]}' by '{matched_gen_pair[1]}') (Score: {best_pair_score:.2f})\")\n",
        "\n",
        "    score = correctly_assigned / total_gold_pairs\n",
        "\n",
        "    unmatched_details = []\n",
        "    for gold_act, gold_acr in gold_pairs:\n",
        "        if not any(f\"Gold ('{gold_act}' by '{gold_acr}')\" in d for d in details):\n",
        "            unmatched_details.append(f\"MISSING: Gold pair ('{gold_act}' by '{gold_acr}') not found in generated model.\")\n",
        "\n",
        "    for idx, (gen_act, gen_acr) in enumerate(gen_pairs):\n",
        "        if idx not in matched_gen_indices:\n",
        "            unmatched_details.append(f\"EXTRA: Generated pair ('{gen_act}' by '{gen_acr}') has no match in gold standard.\")\n",
        "\n",
        "    return score, f\"{correctly_assigned}/{total_gold_pairs}\", \"\\n\".join(details + unmatched_details)\n",
        "\n",
        "\n",
        "def check_control_flow(gold_model, gen_model, model, threshold):\n",
        "    \"\"\"\n",
        "    Checks for consecutive pairs of semantically similar flow nodes.\n",
        "    \"\"\"\n",
        "    gold_pairs = gold_model.get_control_flow_pairs()\n",
        "    gen_pairs = gen_model.get_control_flow_pairs()\n",
        "    total_gold_pairs = len(gold_pairs)\n",
        "\n",
        "    if total_gold_pairs == 0:\n",
        "        return \"NA\" if not gen_pairs else 1.0, \"0/0\", \"Not applicable: No control flow pairs in gold standard.\"\n",
        "\n",
        "    if not gen_pairs:\n",
        "        return 0.0, f\"0/{total_gold_pairs}\", \"No control flow pairs found in the generated model.\"\n",
        "\n",
        "    all_names = list(set([name for pair in gold_pairs + gen_pairs for name in pair]))\n",
        "    name_to_embedding = {name: emb for name, emb in zip(all_names, model.encode(all_names))}\n",
        "\n",
        "    def get_similarity(name1, name2):\n",
        "        if name1 not in name_to_embedding or name2 not in name_to_embedding:\n",
        "            return 0.0\n",
        "        return util.pytorch_cos_sim(name_to_embedding[name1], name_to_embedding[name2]).item()\n",
        "\n",
        "    matched_gen_indices = set()\n",
        "    correct_pairs = 0\n",
        "    details = []\n",
        "\n",
        "    for gold_idx, (gs_name, gt_name) in enumerate(gold_pairs):\n",
        "        best_pair_score = -1\n",
        "        best_gen_idx = -1\n",
        "\n",
        "        for gen_idx, (ms_name, mt_name) in enumerate(gen_pairs):\n",
        "            if gen_idx in matched_gen_indices:\n",
        "                continue\n",
        "\n",
        "            sim_source = get_similarity(gs_name, ms_name)\n",
        "            sim_target = get_similarity(gt_name, mt_name)\n",
        "\n",
        "            if sim_source >= threshold and sim_target >= threshold:\n",
        "                current_pair_score = (sim_source + sim_target) / 2.0\n",
        "                if current_pair_score > best_pair_score:\n",
        "                    best_pair_score = current_pair_score\n",
        "                    best_gen_idx = gen_idx\n",
        "\n",
        "        if best_gen_idx != -1:\n",
        "            correct_pairs += 1\n",
        "            matched_gen_indices.add(best_gen_idx)\n",
        "            matched_gen_pair = gen_pairs[best_gen_idx]\n",
        "            details.append(f\"MATCH: Gold ('{gs_name}' -> '{gt_name}') with Gen ('{matched_gen_pair[0]}' -> '{matched_gen_pair[1]}') (Score: {best_pair_score:.2f})\")\n",
        "\n",
        "    score = correct_pairs / total_gold_pairs\n",
        "\n",
        "    unmatched_details = []\n",
        "    for gs, gt in gold_pairs:\n",
        "        if not any(f\"Gold ('{gs}' -> '{gt}')\" in d for d in details):\n",
        "            unmatched_details.append(f\"MISSING: Gold pair ('{gs}' -> '{gt}') not found in generated model.\")\n",
        "\n",
        "    for idx, (ms, mt) in enumerate(gen_pairs):\n",
        "        if idx not in matched_gen_indices:\n",
        "            unmatched_details.append(f\"EXTRA: Generated pair ('{ms}' -> '{mt}') has no match in the gold standard.\")\n",
        "\n",
        "    return score, f\"{correct_pairs}/{total_gold_pairs}\", \"\\n\".join(details + unmatched_details)\n",
        "\n",
        "\n",
        "def check_decision_logic(gold_model, gen_model, model, threshold):\n",
        "    \"\"\"\n",
        "    Checks if decision splits (gateway + labels) are semantically correct.\n",
        "    \"\"\"\n",
        "    gold_splits = gold_model.get_decision_splits()\n",
        "    gen_splits = gen_model.get_decision_splits()\n",
        "    total_gold_splits = len(gold_splits)\n",
        "\n",
        "    if total_gold_splits == 0:\n",
        "        return \"NA\" if not gen_splits else 1.0, \"0/0\", \"Not applicable: No decision splits in gold standard.\"\n",
        "\n",
        "    if not gen_splits:\n",
        "        return 0.0, f\"0/{total_gold_splits}\", \"No decision splits found in the generated model.\"\n",
        "\n",
        "    all_gw_names = list(set([s['name'] for s in gold_splits] + [s['name'] for s in gen_splits]))\n",
        "    all_labels = list(set([label for s in gold_splits for label in s['labels']] + [label for s in gen_splits for label in s['labels']]))\n",
        "\n",
        "    gw_embeddings = {name: emb for name, emb in zip(all_gw_names, model.encode(all_gw_names))}\n",
        "    label_embeddings = {name: emb for name, emb in zip(all_labels, model.encode(all_labels))}\n",
        "\n",
        "    def get_gw_similarity(name1, name2):\n",
        "        if name1 not in gw_embeddings or name2 not in gw_embeddings: return 0.0\n",
        "        return util.pytorch_cos_sim(gw_embeddings[name1], gw_embeddings[name2]).item()\n",
        "\n",
        "    def get_label_set_recall(gold_labels, gen_labels):\n",
        "        if not gold_labels: return 1.0\n",
        "        if not gen_labels: return 0.0\n",
        "\n",
        "        label_matches = 0\n",
        "        used_gen_indices = set()\n",
        "        for gl in gold_labels:\n",
        "            best_score = -1\n",
        "            best_idx = -1\n",
        "            for i, ml in enumerate(gen_labels):\n",
        "                if i in used_gen_indices: continue\n",
        "                score = util.pytorch_cos_sim(label_embeddings[gl], label_embeddings[ml]).item()\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_idx = i\n",
        "\n",
        "            if best_score >= threshold:\n",
        "                label_matches += 1\n",
        "                if best_idx != -1:\n",
        "                    used_gen_indices.add(best_idx)\n",
        "\n",
        "        return label_matches / len(gold_labels)\n",
        "\n",
        "    correct_splits = 0\n",
        "    matched_gen_indices = set()\n",
        "    details = []\n",
        "\n",
        "    for gold_split in gold_splits:\n",
        "        best_overall_score = -1\n",
        "        best_gen_idx = -1\n",
        "\n",
        "        for gen_idx, gen_split in enumerate(gen_splits):\n",
        "            if gen_idx in matched_gen_indices:\n",
        "                continue\n",
        "\n",
        "            gw_sim = get_gw_similarity(gold_split['name'], gen_split['name'])\n",
        "\n",
        "            if gw_sim >= threshold:\n",
        "                label_recall = get_label_set_recall(gold_split['labels'], gen_split['labels'])\n",
        "                overall_score = (gw_sim + label_recall) / 2.0\n",
        "\n",
        "                if overall_score > best_overall_score:\n",
        "                    best_overall_score = overall_score\n",
        "                    best_gen_idx = gen_idx\n",
        "\n",
        "        if best_gen_idx != -1:\n",
        "            correct_splits += 1\n",
        "            matched_gen_indices.add(best_gen_idx)\n",
        "            matched_gen_split = gen_splits[best_gen_idx]\n",
        "            details.append(f\"MATCH: Gold Split '{gold_split['name']}' with Gen Split '{matched_gen_split['name']}' (Score: {best_overall_score:.2f})\")\n",
        "\n",
        "    score = correct_splits / total_gold_splits\n",
        "\n",
        "    unmatched_details = []\n",
        "    for gold_split in gold_splits:\n",
        "        if not any(f\"'{gold_split['name']}'\" in d for d in details):\n",
        "             unmatched_details.append(f\"MISSING: Gold split '{gold_split['name']}' not found in generated model.\")\n",
        "\n",
        "    for idx, gen_split in enumerate(gen_splits):\n",
        "        if idx not in matched_gen_indices:\n",
        "            unmatched_details.append(f\"EXTRA: Generated split '{gen_split['name']}' has no match in gold standard.\")\n",
        "\n",
        "    return score, f\"{correct_splits}/{total_gold_splits}\", \"\\n\".join(details + unmatched_details)\n",
        "\n",
        "\n",
        "def check_conditional_logic_llm(source_text, gen_model, api_key):\n",
        "    \"\"\"Uses an LLM to evaluate if business rules from text are in the model.\"\"\"\n",
        "    model_description = describe_model_for_llm(gen_model)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert BPMN 2.0 analyst. Your task is to analyze a source text, identify all business rules within it, and then evaluate if each rule is correctly implemented in a given BPMN model.\n",
        "\n",
        "    **Source Text:**\n",
        "    ```\n",
        "    {source_text}\n",
        "    ```\n",
        "\n",
        "    **Description of the Generated BPMN Model:**\n",
        "    ```\n",
        "    {model_description}\n",
        "    ```\n",
        "\n",
        "    **Analysis Task:**\n",
        "    1.  Read the entire Source Text and identify every sentence that constitutes a business rule, a constraint, or a condition (e.g., sentences containing \"must\", \"if\", \"can\", \"within\", etc.).\n",
        "    2.  For each rule you identify, evaluate its implementation in the provided BPMN Model Description.\n",
        "    3.  Consider if conditional clauses use gateways, if temporal constraints use timer events, and if deontic logic (must, can, must not) is correctly modeled.\n",
        "\n",
        "    **Output Format:**\n",
        "    Respond with a single JSON array only. Each object in the array should represent one rule you identified and have three keys: \"identified_rule\" (string), \"evaluation\" (string: \"Correct\" or \"Incorrect\"), and \"justification\" (string).\n",
        "    \"\"\"\n",
        "\n",
        "    response_text = ask_llm(prompt, api_key)\n",
        "    details = []\n",
        "    correct_implementations = 0\n",
        "    total_rules_identified = 0\n",
        "\n",
        "    try:\n",
        "        clean_response = response_text.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
        "        llm_eval_list = json.loads(clean_response)\n",
        "\n",
        "        if not isinstance(llm_eval_list, list):\n",
        "             raise json.JSONDecodeError(\"Response is not a list.\", clean_response, 0)\n",
        "\n",
        "        total_rules_identified = len(llm_eval_list)\n",
        "\n",
        "        for eval_item in llm_eval_list:\n",
        "            rule = eval_item.get(\"identified_rule\", \"N/A\")\n",
        "            evaluation = eval_item.get(\"evaluation\", \"Error\")\n",
        "            justification = eval_item.get(\"justification\", \"No justification provided.\")\n",
        "\n",
        "            if evaluation.lower() == \"correct\":\n",
        "                correct_implementations += 1\n",
        "\n",
        "            details.append(f\"Rule: '{rule}' -> {evaluation}. Justification: {justification}\")\n",
        "\n",
        "    except (json.JSONDecodeError, AttributeError, TypeError) as e:\n",
        "        details.append(f\"Error parsing LLM response: {e}\\nRaw Response: {response_text}\")\n",
        "\n",
        "    score = correct_implementations / total_rules_identified if total_rules_identified > 0 else 1.0\n",
        "    if total_rules_identified == 0:\n",
        "        score = \"NA\" # If no rules are identified, it's not applicable.\n",
        "\n",
        "    return score, f\"{correct_implementations}/{total_rules_identified}\", \"\\n\".join(details)\n",
        "\n",
        "\n",
        "def run_correctness_check(gold_file, gen_file, thresholds, model, source_text, api_key):\n",
        "    \"\"\"Main function to run all correctness checks for a pair of files.\"\"\"\n",
        "    results = {\n",
        "        'Gold Standard': os.path.basename(gold_file),\n",
        "        'Generated Model': os.path.basename(gen_file),\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        gold_model = BPMNModel(gold_file)\n",
        "        gen_model = BPMNModel(gen_file)\n",
        "    except IOError as e:\n",
        "        print(f\"  - SKIPPING: Could not process file. Error: {e}\")\n",
        "        return None\n",
        "\n",
        "    score, absolute, details = check_actor_assignment(gold_model, gen_model, model, thresholds)\n",
        "    results['(i) Actor Assignment Score'] = score\n",
        "    results['(i) Actor Assignment Absolute'] = absolute\n",
        "    results['(i) Actor Assignment Details'] = details\n",
        "\n",
        "    score, absolute, details = check_control_flow(gold_model, gen_model, model, thresholds['activities'])\n",
        "    results['(ii) Control Flow Score'] = score\n",
        "    results['(ii) Control Flow Absolute'] = absolute\n",
        "    results['(ii) Control Flow Details'] = details\n",
        "\n",
        "    score, absolute, details = check_decision_logic(gold_model, gen_model, model, thresholds['conditions'])\n",
        "    results['(iii) Decision Logic Score'] = score\n",
        "    results['(iii) Decision Logic Absolute'] = absolute\n",
        "    results['(iii) Decision Logic Details'] = details\n",
        "\n",
        "    score, absolute, details = check_conditional_logic_llm(source_text, gen_model, api_key)\n",
        "    results['(iv) Conditional Logic (LLM) Score'] = score\n",
        "    results['(iv) Conditional Logic (LLM) Absolute'] = absolute\n",
        "    results['(iv) Conditional Logic (LLM) Details'] = details\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    comparisons = [\n",
        "        {\"gold\": \"/content/smart_meter_refactored.bpmn\", \"gen\": \"/content/smart_meter_01.bpmn\", \"source_text_file\": \"/content/1_smart_meter_uc3_5.txt\"},\n",
        "        {\"gold\": \"/content/smart_meter_refactored.bpmn\", \"gen\": \"/content/smart_meter_02.bpmn\", \"source_text_file\": \"/content/1_smart_meter_uc3_5.txt\"},\n",
        "        {\"gold\": \"/content/smart_meter_refactored.bpmn\", \"gen\": \"/content/smart_meter_03.bpmn\", \"source_text_file\": \"/content/1_smart_meter_uc3_5.txt\"},\n",
        "        {\"gold\": \"/content/gdpr_refactored.bpmn\", \"gen\": \"/content/gdpr_01.bpmn\", \"source_text_file\": \"/content/2_gdpr_article33_34.txt\"},\n",
        "        {\"gold\": \"/content/gdpr_refactored.bpmn\", \"gen\": \"/content/gdpr_02.bpmn\", \"source_text_file\": \"/content/2_gdpr_article33_34.txt\"},\n",
        "        {\"gold\": \"/content/gdpr_refactored.bpmn\", \"gen\": \"/content/gdpr_03.bpmn\", \"source_text_file\": \"/content/2_gdpr_article33_34.txt\"},\n",
        "        {\"gold\": \"/content/blood_donor_selection.bpmn\", \"gen\": \"/content/blood_donor_01.bpmn\", \"source_text_file\": \"/content/3_pdfExtractor_blood_donor_selection.txt\"},\n",
        "        {\"gold\": \"/content/blood_donor_selection.bpmn\", \"gen\": \"/content/blood_donor_02.bpmn\", \"source_text_file\": \"/content/3_pdfExtractor_blood_donor_selection.txt\"},\n",
        "        {\"gold\": \"/content/blood_donor_selection.bpmn\", \"gen\": \"/content/blood_donor_03.bpmn\", \"source_text_file\": \"/content/3_pdfExtractor_blood_donor_selection.txt\"},\n",
        "        {\"gold\": \"/content/health_data.bpmn\", \"gen\": \"/content/health_data_01.bpmn\", \"source_text_file\": \"/content/4_health_data_article32.txt\"},\n",
        "        {\"gold\": \"/content/health_data.bpmn\", \"gen\": \"/content/health_data_02.bpmn\", \"source_text_file\": \"/content/4_health_data_article32.txt\"},\n",
        "        {\"gold\": \"/content/health_data.bpmn\", \"gen\": \"/content/health_data_03.bpmn\", \"source_text_file\": \"/content/4_health_data_article32.txt\"},\n",
        "        {\"gold\": \"/content/finance_customer_due_diligence.bpmn\", \"gen\": \"/content/CDD_01.bpmn\", \"source_text_file\": \"/content/5_CDD.txt\"},\n",
        "        {\"gold\": \"/content/finance_customer_due_diligence.bpmn\", \"gen\": \"/content/CDD_02.bpmn\", \"source_text_file\": \"/content/5_CDD.txt\"},\n",
        "        {\"gold\": \"/content/finance_customer_due_diligence.bpmn\", \"gen\": \"/content/CDD_03.bpmn\", \"source_text_file\": \"/content/5_CDD.txt\"},\n",
        "    ]\n",
        "\n",
        "    output_excel_file = \"bpmn_correctness_report_CARB.xlsx\"\n",
        "\n",
        "    gemini_api_key = None\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        gemini_api_key = userdata.get('Google_API_Key')\n",
        "        if not gemini_api_key:\n",
        "            print(\"\\nWARNING: Could not retrieve 'Google_API_Key' from Colab secrets.\")\n",
        "            gemini_api_key = \"YOUR_API_KEY_HERE\"\n",
        "    except ImportError:\n",
        "        gemini_api_key = \"YOUR_API_KEY_HERE\"\n",
        "\n",
        "    similarity_thresholds = {\n",
        "        'activities': 0.50,\n",
        "        'conditions': 0.50,\n",
        "        'actors': 0.70\n",
        "    }\n",
        "\n",
        "    print(\"Loading semantic similarity model...\")\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "    if not gemini_api_key or gemini_api_key == \"YOUR_API_KEY_HERE\":\n",
        "        print(\"\\nWARNING: Gemini API key is not set. LLM-based checks will fail.\")\n",
        "        print(\"Please edit the script or set up your environment correctly.\\n\")\n",
        "\n",
        "    all_results = []\n",
        "    print(\"\\nStarting correctness check...\")\n",
        "    for item in comparisons:\n",
        "        gold_file = item[\"gold\"]\n",
        "        gen_file = item[\"gen\"]\n",
        "        source_text_file = item[\"source_text_file\"]\n",
        "\n",
        "        try:\n",
        "            with open(source_text_file, 'r', encoding='utf-8') as f:\n",
        "                source_text = f.read()\n",
        "        except FileNotFoundError:\n",
        "            print(f\"  - SKIPPING: Source text file not found: {source_text_file}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"  - SKIPPING: Error reading source text file {source_text_file}: {e}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"  - Comparing '{os.path.basename(gold_file)}' with '{os.path.basename(gen_file)}'\")\n",
        "        result_data = run_correctness_check(gold_file, gen_file, similarity_thresholds, model, source_text, gemini_api_key)\n",
        "        if result_data:\n",
        "            all_results.append(result_data)\n",
        "\n",
        "    if all_results:\n",
        "        df = pd.DataFrame(all_results)\n",
        "        column_order = [\n",
        "            'Gold Standard', 'Generated Model',\n",
        "            '(i) Actor Assignment Score', '(i) Actor Assignment Absolute',\n",
        "            '(ii) Control Flow Score', '(ii) Control Flow Absolute',\n",
        "            '(iii) Decision Logic Score', '(iii) Decision Logic Absolute',\n",
        "            '(iv) Conditional Logic (LLM) Score', '(iv) Conditional Logic (LLM) Absolute',\n",
        "            '(i) Actor Assignment Details', '(ii) Control Flow Details',\n",
        "            '(iii) Decision Logic Details', '(iv) Conditional Logic (LLM) Details'\n",
        "        ]\n",
        "        df = df.reindex(columns=column_order)\n",
        "\n",
        "        try:\n",
        "            df.to_excel(output_excel_file, index=False, engine='openpyxl')\n",
        "            print(f\"\\nSuccessfully exported {len(all_results)} results to '{output_excel_file}'\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError exporting to Excel: {e}\")\n",
        "    else:\n",
        "        print(\"\\nNo results to export.\")\n"
      ],
      "metadata": {
        "id": "DXd_D4Fd0zpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Traceability"
      ],
      "metadata": {
        "id": "hOLS3a-heKUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers torch pandas openpyxl"
      ],
      "metadata": {
        "id": "hAVzOa-AIokh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "NAMESPACES = {'bpmn': 'http://www.omg.org/spec/BPMN/20100524/MODEL'}\n",
        "\n",
        "\n",
        "def get_elements_by_tag(root, tag_name):\n",
        "    \"\"\"\n",
        "    Finds all elements with a specific tag name in the BPMN XML tree.\n",
        "\n",
        "    Args:\n",
        "        root (ET.Element): The root of the XML tree.\n",
        "        tag_name (str): The BPMN tag to search for (e.g., 'task', 'userTask').\n",
        "\n",
        "    Returns:\n",
        "        list: A list of found XML elements.\n",
        "    \"\"\"\n",
        "    return root.findall(f'.//bpmn:{tag_name}', NAMESPACES)\n",
        "\n",
        "def get_element_names(elements):\n",
        "    \"\"\"\n",
        "    Extracts the 'name' attribute from a list of BPMN elements.\n",
        "    Filters out elements without a name.\n",
        "\n",
        "    Args:\n",
        "        elements (list): A list of XML elements.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of names (strings).\n",
        "    \"\"\"\n",
        "    names = [elem.get('name') for elem in elements]\n",
        "    return [name for name in names if name]\n",
        "\n",
        "def get_actors(root):\n",
        "    \"\"\"\n",
        "    Extracts actors from the BPMN model. Actors are represented as\n",
        "    Participants (which correspond to Pools) and Lanes.\n",
        "\n",
        "    Args:\n",
        "        root (ET.Element): The root of the XML tree.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of actor names.\n",
        "    \"\"\"\n",
        "    participants = get_elements_by_tag(root, 'participant')\n",
        "    lanes = get_elements_by_tag(root, 'lane')\n",
        "    return get_element_names(participants + lanes)\n",
        "\n",
        "\n",
        "def get_activities(root):\n",
        "    \"\"\"\n",
        "    Extracts activities from the BPMN model.\n",
        "    This includes tasks, user tasks, service tasks, etc.\n",
        "\n",
        "    Args:\n",
        "        root (ET.Element): The root of the XML tree.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of activity names.\n",
        "    \"\"\"\n",
        "    activity_tags = ['task', 'userTask', 'serviceTask', 'sendTask', 'receiveTask',\n",
        "                     'manualTask', 'businessRuleTask', 'scriptTask', 'callActivity', 'subProcess']\n",
        "    activities = []\n",
        "    for tag in activity_tags:\n",
        "        activities.extend(get_elements_by_tag(root, tag))\n",
        "    return get_element_names(activities)\n",
        "\n",
        "\n",
        "\n",
        "def calculate_precision_for_semantic_elements(gold_standard_names, generated_names, similarity_threshold, model):\n",
        "    \"\"\"\n",
        "    Calculates precision for named elements based on semantic similarity and returns detailed match lists.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing (precision, matches_count, total_generated, matches_list, unmatched_gold, unmatched_generated).\n",
        "    \"\"\"\n",
        "    if not generated_names:\n",
        "        return 1.0, 0, 0, [], gold_standard_names, []\n",
        "    if not gold_standard_names:\n",
        "        return 0.0, 0, len(generated_names), [], [], generated_names\n",
        "\n",
        "    gold_embeddings = model.encode(gold_standard_names, convert_to_tensor=True)\n",
        "    generated_embeddings = model.encode(generated_names, convert_to_tensor=True)\n",
        "\n",
        "    cosine_scores = util.pytorch_cos_sim(gold_embeddings, generated_embeddings)\n",
        "\n",
        "    matches_list = []\n",
        "    unmatched_gold_indices = set(range(len(gold_standard_names)))\n",
        "    unmatched_generated_indices = set(range(len(generated_names)))\n",
        "\n",
        "    matched_generated_indices = set()\n",
        "\n",
        "    for gs_idx in range(len(gold_standard_names)):\n",
        "        best_score = -1\n",
        "        best_gen_idx = -1\n",
        "        for gen_idx in range(len(generated_names)):\n",
        "            if gen_idx in matched_generated_indices:\n",
        "                continue\n",
        "\n",
        "            score = cosine_scores[gs_idx][gen_idx]\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_gen_idx = gen_idx\n",
        "\n",
        "        if best_score >= similarity_threshold:\n",
        "            matches_list.append(\n",
        "                (gold_standard_names[gs_idx], generated_names[best_gen_idx], best_score.item())\n",
        "            )\n",
        "            if best_gen_idx != -1:\n",
        "                matched_generated_indices.add(best_gen_idx)\n",
        "\n",
        "            if gs_idx in unmatched_gold_indices:\n",
        "                unmatched_gold_indices.remove(gs_idx)\n",
        "            if best_gen_idx in unmatched_generated_indices:\n",
        "                unmatched_generated_indices.remove(best_gen_idx)\n",
        "\n",
        "    matches_count = len(matches_list)\n",
        "    total_generated = len(generated_names)\n",
        "    precision = matches_count / total_generated if total_generated > 0 else 1.0\n",
        "\n",
        "    unmatched_gold = [gold_standard_names[i] for i in unmatched_gold_indices]\n",
        "    unmatched_generated = [generated_names[i] for i in unmatched_generated_indices]\n",
        "\n",
        "    return precision, matches_count, total_generated, matches_list, unmatched_gold, unmatched_generated\n",
        "\n",
        "\n",
        "def format_matches_for_excel(matches_list, unmatched_gold, unmatched_gen):\n",
        "    \"\"\"Formats the detailed match lists into a single string for an Excel cell.\"\"\"\n",
        "    report_parts = []\n",
        "    if matches_list:\n",
        "        report_parts.append(\"MATCHED (Correctly Generated):\")\n",
        "        for gs, gen, score in sorted(matches_list, key=lambda x: x[2], reverse=True):\n",
        "            report_parts.append(f\"  - G: '{gs}' <-> M: '{gen}' ({score:.2f})\")\n",
        "    if unmatched_gen:\n",
        "        report_parts.append(\"\\nEXTRA IN MODEL (Incorrectly Generated):\")\n",
        "        for name in sorted(unmatched_gen):\n",
        "            report_parts.append(f\"  - {name}\")\n",
        "    if unmatched_gold:\n",
        "        report_parts.append(\"\\nMISSING FROM MODEL (Not relevant for precision but good to know):\")\n",
        "        for name in sorted(unmatched_gold):\n",
        "            report_parts.append(f\"  - {name}\")\n",
        "    return \"\\n\".join(report_parts)\n",
        "\n",
        "\n",
        "def run_traceability_check(gold_standard_file, generated_file, thresholds, model):\n",
        "    \"\"\"\n",
        "    Main function to run the traceability check (precision).\n",
        "    Returns a dictionary with the precision results.\n",
        "    \"\"\"\n",
        "    results = {\n",
        "        'Gold Standard': os.path.basename(gold_standard_file),\n",
        "        'Generated Model': os.path.basename(generated_file),\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        gold_tree = ET.parse(gold_standard_file)\n",
        "        gold_root = gold_tree.getroot()\n",
        "    except (FileNotFoundError, ET.ParseError) as e:\n",
        "        print(f\"Error reading gold standard file '{gold_standard_file}': {e}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        generated_tree = ET.parse(generated_file)\n",
        "        generated_root = generated_tree.getroot()\n",
        "    except (FileNotFoundError, ET.ParseError) as e:\n",
        "        print(f\"Error reading generated file '{generated_file}': {e}\")\n",
        "        return None\n",
        "\n",
        "    traceability_elements = {\n",
        "        'Actors': (get_actors(gold_root), get_actors(generated_root)),\n",
        "        'Activities': (get_activities(gold_root), get_activities(generated_root)),\n",
        "    }\n",
        "\n",
        "    for category, (gold_list, gen_list) in traceability_elements.items():\n",
        "        thresh_key = category.lower().replace(' ', '_')\n",
        "        thresh = thresholds.get(thresh_key, 0.75)\n",
        "        precision, matches, total_generated, matches_list, unmatched_gold, unmatched_gen = \\\n",
        "            calculate_precision_for_semantic_elements(gold_list, gen_list, thresh, model)\n",
        "\n",
        "        results[f'{category} Precision'] = precision\n",
        "        results[f'{category} Absolute (Precision)'] = f\"{matches}/{total_generated}\"\n",
        "        results[f'{category} Details'] = format_matches_for_excel(matches_list, unmatched_gold, unmatched_gen)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    comparison_files = [\n",
        "        (\"/content/smart_meter_refactored.bpmn\", \"/content/smart_meter_01.bpmn\"),\n",
        "        (\"/content/smart_meter_refactored.bpmn\", \"/content/smart_meter_02.bpmn\"),\n",
        "        (\"/content/smart_meter_refactored.bpmn\", \"/content/smart_meter_03.bpmn\"),\n",
        "        (\"/content/gdpr_refactored.bpmn\", \"/content/gdpr_01.bpmn\"),\n",
        "        (\"/content/gdpr_refactored.bpmn\", \"/content/gdpr_02.bpmn\"),\n",
        "        (\"/content/gdpr_refactored.bpmn\", \"/content/gdpr_03.bpmn\"),\n",
        "        (\"/content/blood_donor_selection.bpmn\", \"/content/blood_donor_01.bpmn\"),\n",
        "        (\"/content/blood_donor_selection.bpmn\", \"/content/blood_donor_02.bpmn\"),\n",
        "        (\"/content/blood_donor_selection.bpmn\", \"/content/blood_donor_03.bpmn\"),\n",
        "        (\"/content/health_data.bpmn\", \"/content/health_data_01.bpmn\"),\n",
        "        (\"/content/health_data.bpmn\", \"/content/health_data_02.bpmn\"),\n",
        "        (\"/content/health_data.bpmn\", \"/content/health_data_03.bpmn\"),\n",
        "        (\"/content/finance_customer_due_diligence.bpmn\", \"/content/CDD_01.bpmn\"),\n",
        "        (\"/content/finance_customer_due_diligence.bpmn\", \"/content/CDD_02.bpmn\"),\n",
        "        (\"/content/finance_customer_due_diligence.bpmn\", \"/content/CDD_03.bpmn\")\n",
        "    ]\n",
        "\n",
        "    output_excel_file = \"bpmn_traceability_report_CARB.xlsx\"\n",
        "\n",
        "    similarity_thresholds = {\n",
        "        'actors': 0.70,\n",
        "        'activities': 0.5\n",
        "    }\n",
        "\n",
        "    print(\"Loading semantic similarity model (this may take a moment)...\")\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "    all_results = []\n",
        "    print(\"\\nStarting traceability (precision) check...\")\n",
        "    for gold_file, generated_file in comparison_files:\n",
        "        print(f\"  - Comparing '{os.path.basename(gold_file)}' with '{os.path.basename(generated_file)}'\")\n",
        "        result_data = run_traceability_check(gold_file, generated_file, similarity_thresholds, model)\n",
        "        if result_data:\n",
        "            all_results.append(result_data)\n",
        "\n",
        "    if all_results:\n",
        "        df = pd.DataFrame(all_results)\n",
        "\n",
        "        column_order = [\n",
        "            'Gold Standard', 'Generated Model',\n",
        "            'Actors Precision', 'Actors Absolute (Precision)',\n",
        "            'Activities Precision', 'Activities Absolute (Precision)',\n",
        "            'Actors Details', 'Activities Details'\n",
        "        ]\n",
        "        df = df.reindex(columns=column_order)\n",
        "\n",
        "\n",
        "        try:\n",
        "            df.to_excel(output_excel_file, index=False, engine='openpyxl')\n",
        "            print(f\"\\nSuccessfully exported {len(all_results)} results to '{output_excel_file}'\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError exporting to Excel: {e}\")\n",
        "    else:\n",
        "        print(\"\\nNo results to export.\")"
      ],
      "metadata": {
        "id": "jh8OAF3YZMh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# III. Layout Eval"
      ],
      "metadata": {
        "id": "6b1dtAan-xOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas openpyxl requests"
      ],
      "metadata": {
        "id": "Rxehv3AuGMtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "import base64\n",
        "\n",
        "def encode_image_to_base64(image_path):\n",
        "    \"\"\"Reads an image file and encodes it into a Base64 string.\"\"\"\n",
        "    try:\n",
        "        with open(image_path, \"rb\") as image_file:\n",
        "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "    except IOError as e:\n",
        "        print(f\"Error reading image file {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def ask_llm_for_layout_analysis(image_base64, api_key):\n",
        "\n",
        "    api_url = f\"https://generativelanguage.googleapis.com/v1/models/gemini-2.5-pro:generateContent?key={api_key}\"\n",
        "\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "    prompt = \"\"\"\n",
        "    You are a meticulous BPMN 2.0 layout quality analyst. Your task is to analyze the provided BPMN diagram image and identify all layout issues.\n",
        "\n",
        "    **Analysis Criteria:**\n",
        "    Please check for the all three of the following categories of issues:\n",
        "    1.  **Text Overlaps:** Any text label that overlaps with another label or a BPMN element's (e.g. pools) boundary.\n",
        "    2.  **Element Overlaps:** Any BPMN shapes (tasks, events, gateways) that overlap each other. A task must be fully contained within its lane. Sequence flows (solid lines) must not cross over tasks or other elements. Note: Message flows (dashed lines) are permitted to cross over pools.\n",
        "    3.  **Non-Orthogonal Edges:** Any sequence flow (solid line with a solid arrowhead) that is not horizontal or vertical. Diagonal or curved sequence flows are considered layout issues.\n",
        "\n",
        "    **Output Format:**\n",
        "    Respond with a single JSON object only. Do not include any other text or markdown formatting.\n",
        "    The JSON object should have two keys:\n",
        "    - \"overall_score\": A single float from 0.0 (very poor) to 1.0 (perfect), representing the overall layout quality.\n",
        "    - \"identified_issues\": A list of objects. Each object must have two keys: \"issue_description\" (a clear, concise description of a single layout problem) and \"severity\" (a string: \"High\", \"Medium\", or \"Low\"). If there are no issues, return an empty list.\n",
        "\n",
        "    **Example Response:**\n",
        "    {\n",
        "      \"overall_score\": 0.65,\n",
        "      \"identified_issues\": [\n",
        "        {\n",
        "          \"issue_description\": \"The label for 'Validate Order' task overlaps with the task boundary.\",\n",
        "          \"severity\": \"Medium\"\n",
        "        },\n",
        "        {\n",
        "          \"issue_description\": \"The sequence flow from 'Approve Order' to 'Ship Order' is drawn diagonally.\",\n",
        "          \"severity\": \"High\"\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    payload = {\n",
        "        \"contents\": [{\n",
        "            \"parts\": [\n",
        "                {\"text\": prompt},\n",
        "                {\n",
        "                    \"inline_data\": {\n",
        "                        \"mime_type\": \"image/png\",\n",
        "                        \"data\": image_base64\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "        }]\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(api_url, headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        response_json = response.json()\n",
        "\n",
        "        if 'candidates' in response_json and len(response_json['candidates']) > 0:\n",
        "            content = response_json['candidates'][0].get('content', {})\n",
        "            if 'parts' in content and len(content['parts']) > 0:\n",
        "                return content['parts'][0].get('text', '')\n",
        "        return '{\"error\": \"Could not parse LLM response.\"}'\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"API Request Error: {e}\")\n",
        "        return f'{{\"error\": \"API request failed: {e}\"}}'\n",
        "    finally:\n",
        "        time.sleep(1)\n",
        "\n",
        "\n",
        "def run_layout_check(image_path, api_key):\n",
        "    \"\"\"\n",
        "    Main function to run the layout check for a single image.\n",
        "    Returns a dictionary with the results.\n",
        "    \"\"\"\n",
        "    results = {\n",
        "        'Model Image': os.path.basename(image_path),\n",
        "        'Overall Score': None,\n",
        "        'Issues Count': 0,\n",
        "        'Issues Details': 'N/A'\n",
        "    }\n",
        "\n",
        "    image_base64 = encode_image_to_base64(image_path)\n",
        "    if not image_base64:\n",
        "        results['Issues Details'] = \"Error: Could not read or encode image file.\"\n",
        "        return results\n",
        "\n",
        "    response_text = ask_llm_for_layout_analysis(image_base64, api_key)\n",
        "\n",
        "    try:\n",
        "        clean_response = response_text.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
        "        llm_eval = json.loads(clean_response)\n",
        "\n",
        "        if \"error\" in llm_eval:\n",
        "             raise ValueError(llm_eval[\"error\"])\n",
        "\n",
        "        results['Overall Score'] = llm_eval.get('overall_score', 0.0)\n",
        "        issues = llm_eval.get('identified_issues', [])\n",
        "        results['Issues Count'] = len(issues)\n",
        "\n",
        "        if issues:\n",
        "            details_list = [f\"- {item.get('issue_description', 'N/A')} (Severity: {item.get('severity', 'N/A')})\" for item in issues]\n",
        "            results['Issues Details'] = \"\\n\".join(details_list)\n",
        "        else:\n",
        "            results['Issues Details'] = \"No layout issues identified.\"\n",
        "\n",
        "    except (json.JSONDecodeError, AttributeError, ValueError) as e:\n",
        "        results['Issues Details'] = f\"Error parsing LLM response: {e}\\nRaw Response: {response_text}\"\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    image_files_to_check = [\n",
        "        \"/content/smart_meter_01.png\",\n",
        "        \"/content/gdpr_01.png\",\n",
        "        \"/content/blood_donor_01.png\",\n",
        "        \"/content/health_data_01.png\",\n",
        "        \"/content/CDD_01.png\"\n",
        "    ]\n",
        "\n",
        "    output_excel_file = \"bpmn_layout_report_CARB.xlsx\"\n",
        "\n",
        "    gemini_api_key = None\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        gemini_api_key = userdata.get('Google_API_Key')\n",
        "        if not gemini_api_key:\n",
        "            print(\"\\nWARNING: Could not retrieve 'Google_API_Key' from Colab secrets.\")\n",
        "            print(\"Please ensure the secret is created and has the correct name.\\n\")\n",
        "            gemini_api_key = \"YOUR_API_KEY_HERE\" # Fallback\n",
        "    except ImportError:\n",
        "        print(\"\\nINFO: Not running in a Google Colab environment. Using placeholder for API key.\")\n",
        "        gemini_api_key = \"YOUR_API_KEY_HERE\"\n",
        "\n",
        "    if gemini_api_key == \"YOUR_API_KEY_HERE\":\n",
        "        print(\"\\nWARNING: Gemini API key is not set. The script will not be able to run.\")\n",
        "        print(\"Please edit the script or set up your environment correctly.\\n\")\n",
        "\n",
        "    all_results = []\n",
        "    print(\"\\nStarting BPMN layout analysis...\")\n",
        "    for image_file in image_files_to_check:\n",
        "        if not os.path.exists(image_file):\n",
        "            print(f\"  - Skipping '{image_file}' (File not found).\")\n",
        "            continue\n",
        "\n",
        "        print(f\"  - Analyzing '{os.path.basename(image_file)}'...\")\n",
        "        result_data = run_layout_check(image_file, gemini_api_key)\n",
        "        if result_data:\n",
        "            all_results.append(result_data)\n",
        "\n",
        "    if all_results:\n",
        "        df = pd.DataFrame(all_results)\n",
        "\n",
        "        column_order = [\n",
        "            'Model Image', 'Overall Score', 'Issues Count', 'Issues Details'\n",
        "        ]\n",
        "        df = df.reindex(columns=column_order)\n",
        "\n",
        "        try:\n",
        "            df.to_excel(output_excel_file, index=False, engine='openpyxl')\n",
        "            print(f\"\\nSuccessfully exported {len(all_results)} results to '{output_excel_file}'\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError exporting to Excel: {e}\")\n",
        "    else:\n",
        "        print(\"\\nNo results to export.\")"
      ],
      "metadata": {
        "id": "p4tpMAOlZMjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IV. Various Helper"
      ],
      "metadata": {
        "id": "qUzh3336ZNhf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[A] extracting text from .pdf"
      ],
      "metadata": {
        "id": "4mHEc7_GcE_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "CyGB2jWcZ8zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import os\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a given PDF file.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): The full path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "        str: The extracted text from the PDF, or an error message if something goes wrong.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(pdf_path):\n",
        "        return f\"Error: The file '{pdf_path}' was not found.\"\n",
        "\n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as pdf_file:\n",
        "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "\n",
        "            full_text = \"\"\n",
        "\n",
        "            for page_num in range(len(pdf_reader.pages)):\n",
        "                page = pdf_reader.pages[page_num]\n",
        "\n",
        "                text = page.extract_text()\n",
        "\n",
        "                if text:\n",
        "                    full_text += text + \"\\n--- PAGE BREAK ---\\n\"\n",
        "\n",
        "            return full_text\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred while processing the PDF: {e}\"\n",
        "\n",
        "def save_text_to_file(text, output_filename=\"extracted_text.txt\"):\n",
        "    \"\"\"\n",
        "    Saves the given text to a .txt file.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text content to save.\n",
        "        output_filename (str): The name of the output file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(text)\n",
        "        print(f\"Successfully saved extracted text to '{output_filename}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not save text to file: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pdf_file_path = \"2_blood_donor_selection.pdf\"\n",
        "\n",
        "    extracted_text = extract_text_from_pdf(pdf_file_path)\n",
        "\n",
        "    if \"Error:\" not in extracted_text and \"An error occurred\" not in extracted_text:\n",
        "        save_text_to_file(extracted_text)\n",
        "    else:\n",
        "        print(extracted_text)"
      ],
      "metadata": {
        "id": "eCZYkggUZN7J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}